{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reneemanzari-ship-it/stock-market-news-sentiment-analysis/blob/main/Stock_Market_News_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSufQJjxQWV4"
      },
      "id": "HSufQJjxQWV4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "EvCcfwuSU-fz",
      "metadata": {
        "id": "EvCcfwuSU-fz"
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6QR_RHvIVHT2",
      "metadata": {
        "id": "6QR_RHvIVHT2"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pl3dmH-EnJGl",
      "metadata": {
        "id": "pl3dmH-EnJGl"
      },
      "source": [
        "The prices of the stocks of companies listed under a global exchange are influenced by a variety of factors, with the company's financial performance, innovations and collaborations, and market sentiment being factors that play a significant role. News and media reports can rapidly affect investor perceptions and, consequently, stock prices in the highly competitive financial industry. With the sheer volume of news and opinions from a wide variety of sources, investors and financial analysts often struggle to stay updated and accurately interpret its impact on the market. As a result, investment firms need sophisticated tools to analyze market sentiment and integrate this information into their investment strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vn6bbxSwVKl3",
      "metadata": {
        "id": "Vn6bbxSwVKl3"
      },
      "source": [
        "### Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jCIswL3zobj6",
      "metadata": {
        "id": "jCIswL3zobj6"
      },
      "source": [
        "With an ever-rising number of news articles and opinions, an investment startup aims to leverage artificial intelligence to address the challenge of interpreting stock-related news and its impact on stock prices. They have collected historical daily news for a specific company listed under NASDAQ, along with data on its daily stock price and trade volumes.\n",
        "\n",
        "As a member of the Data Science and AI team in the startup, you have been tasked with developing an AI-driven sentiment analysis system that will automatically process and analyze news articles to gauge market sentiment, and summarizing the news at a weekly level to enhance the accuracy of their stock price predictions and optimize investment strategies. This will empower their financial analysts with actionable insights, leading to more informed investment decisions and improved client outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZJOtDHVSF5hu",
      "metadata": {
        "id": "ZJOtDHVSF5hu"
      },
      "source": [
        "### Data Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZlkjI8V5F9RK",
      "metadata": {
        "id": "ZlkjI8V5F9RK"
      },
      "source": [
        "* `Date` : The date the news was released\n",
        "* `News` : The content of news articles that could potentially affect the company's stock price\n",
        "* `Open` : The stock price (in \\$) at the beginning of the day\n",
        "* `High` : The highest stock price (in \\$) reached during the day\n",
        "* `Low` :  The lowest stock price (in \\$) reached during the day\n",
        "* `Close` : The adjusted stock price (in \\$) at the end of the day\n",
        "* `Volume` : The number of shares traded during the day\n",
        "* `Label` : The sentiment polarity of the news content\n",
        "    * 1: positive\n",
        "    * 0: neutral\n",
        "    * -1: negative"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please read the instructions carefully before starting the project.**"
      ],
      "metadata": {
        "id": "EWGlpDNkrTqA"
      },
      "id": "EWGlpDNkrTqA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_____' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_____' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same.\n",
        "    * Any mathematical or computational details which are a graded part of the project can be included in the Appendix section of the presentation."
      ],
      "metadata": {
        "id": "oIAAXUOip1Fc"
      },
      "id": "oIAAXUOip1Fc"
    },
    {
      "cell_type": "markdown",
      "id": "VrFQHcW5mYgv",
      "metadata": {
        "id": "VrFQHcW5mYgv"
      },
      "source": [
        "## **Installing and Importing the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A-E2-iaumpo8",
      "metadata": {
        "id": "A-E2-iaumpo8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# installing the sentence-transformers and gensim libraries for word embeddings\n",
        "!pip install numpy==1.26.4 \\\n",
        "             scikit-learn==1.6.1 \\\n",
        "             scipy==1.13.1 \\\n",
        "             gensim==4.3.3 \\\n",
        "             sentence-transformers==3.4.1 \\\n",
        "             pandas==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."
      ],
      "metadata": {
        "id": "Su4_EiqL5aIZ"
      },
      "id": "Su4_EiqL5aIZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179a2a45",
      "metadata": {
        "id": "179a2a45"
      },
      "outputs": [],
      "source": [
        "# To manipulate and analyze data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# To visualize data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To used time-related functions\n",
        "import time\n",
        "\n",
        "# To build, tune, and evaluate ML models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# To load/create word embeddings\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# To work with transformer models\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Import TensorFlow and Keras for deep learning model building.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# To implement progress bar related functionalities\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# To ignore unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wQ46zPgumfjF",
      "metadata": {
        "id": "wQ46zPgumfjF"
      },
      "source": [
        "## **Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yu_7XbWQWma8",
      "metadata": {
        "id": "yu_7XbWQWma8"
      },
      "outputs": [],
      "source": [
        "# # uncomment and run the following code if Google Colab is being used and the dataset is in Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a33eef",
      "metadata": {
        "id": "62a33eef"
      },
      "outputs": [],
      "source": [
        "stock_news = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stock_news.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xFSwCCer1uA",
      "metadata": {
        "id": "1xFSwCCer1uA"
      },
      "outputs": [],
      "source": [
        "#Creating a copy of the dataset\n",
        "stock = stock_news.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EvFNfrvGWthn",
      "metadata": {
        "id": "EvFNfrvGWthn"
      },
      "source": [
        "## **Data Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GW4rkWI1WzBb",
      "metadata": {
        "id": "GW4rkWI1WzBb"
      },
      "source": [
        "#### **Displaying the first few rows of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2f105b",
      "metadata": {
        "id": "dd2f105b"
      },
      "outputs": [],
      "source": [
        "stock.head() # Complete the code to check the first 5 rows of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y2ewB36LL9Cz",
      "metadata": {
        "id": "y2ewB36LL9Cz"
      },
      "source": [
        "#### **Understanding the shape of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wWx6wqN0MTPw",
      "metadata": {
        "id": "wWx6wqN0MTPw"
      },
      "outputs": [],
      "source": [
        "stock.shape # Complete the code to check the shape of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fPLJXhFcMA7N",
      "metadata": {
        "id": "fPLJXhFcMA7N"
      },
      "source": [
        "#### **Checking the data types of the columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gc_eAiMdMVe2",
      "metadata": {
        "id": "Gc_eAiMdMVe2"
      },
      "outputs": [],
      "source": [
        "stock.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZD5fstuv6ery",
      "metadata": {
        "id": "ZD5fstuv6ery"
      },
      "outputs": [],
      "source": [
        "# This conversion is now handled after the last data load in cell c32d69d8\n",
        "# stock['Date'] = pd.to_datetime(stock['Date'], format='%m-%d-%Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dORemydMDfR",
      "metadata": {
        "id": "8dORemydMDfR"
      },
      "source": [
        "#### **Checking the statistical summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gUazWjegMeQl",
      "metadata": {
        "id": "gUazWjegMeQl"
      },
      "outputs": [],
      "source": [
        "stock.describe().T # Complete the code to check the statistical summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lXRpNWnQMGIY",
      "metadata": {
        "id": "lXRpNWnQMGIY"
      },
      "source": [
        "#### **Checking the duplicate values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ti4UpPi6M5kM",
      "metadata": {
        "id": "ti4UpPi6M5kM"
      },
      "outputs": [],
      "source": [
        "stock.duplicated().sum() #Complete the code to check the duplicate values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fxghULa0MOY-",
      "metadata": {
        "id": "fxghULa0MOY-"
      },
      "source": [
        "#### **Checking for missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yItWheKoNGkf",
      "metadata": {
        "id": "yItWheKoNGkf"
      },
      "outputs": [],
      "source": [
        "stock.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hGHBK8-QeKOB",
      "metadata": {
        "id": "hGHBK8-QeKOB"
      },
      "source": [
        "## **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q0UlMQnyegl7",
      "metadata": {
        "id": "Q0UlMQnyegl7"
      },
      "source": [
        "### **Univariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RrznHeBaLu0W",
      "metadata": {
        "id": "RrznHeBaLu0W"
      },
      "source": [
        "#### **Countplot on Label**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "meVjTKoxLpmA",
      "metadata": {
        "id": "meVjTKoxLpmA"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=stock, x='Label', stat=\"percent\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dpGHhbGeeoF8",
      "metadata": {
        "id": "dpGHhbGeeoF8"
      },
      "source": [
        "#### **Density Plot of Price (Open, High, Low, Close)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BKqgbg0_v5EM",
      "metadata": {
        "id": "BKqgbg0_v5EM"
      },
      "outputs": [],
      "source": [
        "# Plot KDE for the 'Open', 'High', 'Low', 'Close' columns of the 'stock' DataFrame.\n",
        "sns.displot(data=stock[['Open','High','Low','Close']], kind='kde', palette=\"tab10\");  # Create a KDE plot with a color palette."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Histogram on Volume**"
      ],
      "metadata": {
        "id": "1wBKKuVIaWnl"
      },
      "id": "1wBKKuVIaWnl"
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(stock, x='Volume'); # Complete the code to plot a histogram of Volume"
      ],
      "metadata": {
        "id": "FMDJ_m6maaoK"
      },
      "id": "FMDJ_m6maaoK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9GVt_AAbe29X",
      "metadata": {
        "id": "9GVt_AAbe29X"
      },
      "source": [
        "#### **Histogram and statistical summary on News Length**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kwZSJvwOUpa",
      "metadata": {
        "id": "0kwZSJvwOUpa"
      },
      "outputs": [],
      "source": [
        "#Calculating the total number of words present in the news content.\n",
        "stock['news_len'] = stock['News'].apply(lambda x: len(x.split(' ')))\n",
        "\n",
        "stock['news_len'].describe()    #Complete the code to print the statistical summary for the news content length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NWn03B4Xey5d",
      "metadata": {
        "id": "NWn03B4Xey5d"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data=stock, x=\"news_len\"); #Complete the code to plot a histogram of news_len column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hLE0s7OFKilB",
      "metadata": {
        "id": "hLE0s7OFKilB"
      },
      "source": [
        "### **Bivariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yn_9wfzxL-r1",
      "metadata": {
        "id": "Yn_9wfzxL-r1"
      },
      "source": [
        "#### **Correlation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gOBaxNZeKllB",
      "metadata": {
        "id": "gOBaxNZeKllB"
      },
      "outputs": [],
      "source": [
        "cols = ['Open','High','Low','Close','Volume','news_len']\n",
        "sns.heatmap(\n",
        "    stock[cols].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h-Hz7CpdMAi3",
      "metadata": {
        "id": "h-Hz7CpdMAi3"
      },
      "source": [
        "#### **Label vs Price (Open, High, Low, Close)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i, variable in enumerate(['Open', 'High', 'Low', 'Close']):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    sns.boxplot(data=stock, x=\"Label\", y=variable)\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lCVHNWhgMElU"
      },
      "id": "lCVHNWhgMElU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cY9P2rdBMH-h",
      "metadata": {
        "id": "cY9P2rdBMH-h"
      },
      "source": [
        "#### **Label vs Volume**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mzCxLFg1LCPk",
      "metadata": {
        "id": "mzCxLFg1LCPk"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(\n",
        "    data=stock, x=\"Label\", y=\"Volume\"  # Complete the code to plot a boxplot of Label vs Volume\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ySUmJUyQ0vi",
      "metadata": {
        "id": "9ySUmJUyQ0vi"
      },
      "source": [
        "#### **Date vs Price (Open, High, Low, Close)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tq0NL64DQ0v1",
      "metadata": {
        "id": "tq0NL64DQ0v1"
      },
      "source": [
        "- The data is at the level of news, and we might have more than one news in a day. However, the prices are at daily level\n",
        "- So, we can aggregate the data at a daily level by taking the mean of the attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bECqvVQtwheA",
      "metadata": {
        "id": "bECqvVQtwheA"
      },
      "outputs": [],
      "source": [
        "stock_daily = stock.groupby('Date').agg(\n",
        "    {\n",
        "        'Open': 'mean',\n",
        "        'High': 'mean',\n",
        "        'Low': 'mean',\n",
        "        'Close': 'mean',\n",
        "        'Volume': 'mean',\n",
        "    }\n",
        ").reset_index()  # Group the 'stocks' DataFrame by the 'Date' column\n",
        "\n",
        "stock_daily.set_index('Date', inplace=True)\n",
        "stock_daily.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ORSmC3lxrwy",
      "metadata": {
        "id": "7ORSmC3lxrwy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "sns.lineplot(stock_daily.drop(\"Volume\", axis=1)); #Complete the code to plot a lineplot of all the variables except Volume"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KG4y9NK1Ng1-",
      "metadata": {
        "id": "KG4y9NK1Ng1-"
      },
      "source": [
        "#### **Volume vs Close Price**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0WMHYw6w0TM6",
      "metadata": {
        "id": "0WMHYw6w0TM6"
      },
      "outputs": [],
      "source": [
        "# Create a figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "\n",
        "# Lineplot on primary y-axis\n",
        "sns.lineplot(data=stock_daily.reset_index(), x='Date', y='Close', ax=ax1, color='blue', marker='o', label='Close Price')\n",
        "\n",
        "# Create a secondary y-axis\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Lineplot on secondary y-axis\n",
        "sns.lineplot(data=stock_daily.reset_index(), x='Date', y='Volume', ax=ax2, color='gray', marker='o', label='Volume')\n",
        "\n",
        "ax1.legend(bbox_to_anchor=(1,1));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N8z4-vOBmwqv",
      "metadata": {
        "id": "N8z4-vOBmwqv"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2jIN9NycxtUC",
      "metadata": {
        "id": "2jIN9NycxtUC"
      },
      "outputs": [],
      "source": [
        "stock['Date'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e168fc57"
      },
      "source": [
        "stock_news = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stock_news.csv')"
      ],
      "id": "e168fc57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c32d69d8"
      },
      "source": [
        "stock = stock_news.copy()\n",
        "# Convert the 'Date' column in the 'stock' DataFrame to datetime format after copying\n",
        "stock['Date'] = pd.to_datetime(stock['Date'], format='%m-%d-%Y')"
      ],
      "id": "c32d69d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "j7KR_HgZRDtk",
      "metadata": {
        "id": "j7KR_HgZRDtk"
      },
      "source": [
        "### Train-test Split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stock['Date'].dtype)"
      ],
      "metadata": {
        "id": "ookqz0E9rrcV"
      },
      "id": "ookqz0E9rrcV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yXsgpkpeI8UK",
      "metadata": {
        "id": "yXsgpkpeI8UK"
      },
      "outputs": [],
      "source": [
        "# Select training data: choose all rows where the 'Date' is before a specific date of your choice.\n",
        "# Pick a date that helps split the data into a reasonable train–test ratio for your model (format: 'yyyy-mm-dd').\n",
        "cutoff_date = pd.Timestamp('2019-03-25')\n",
        "X_train = stock[stock['Date'] < cutoff_date].reset_index(drop=True)\n",
        "\n",
        "# Select testing data: choose all rows where the 'Date' is before a specific date of your choice.\n",
        "# Pick a date that helps split the data into a reasonable train–test ratio for your model (format: 'yyyy-mm-dd').\n",
        "X_test = stock[stock['Date'] >= cutoff_date].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__2ON8RuI8Q2",
      "metadata": {
        "id": "__2ON8RuI8Q2"
      },
      "outputs": [],
      "source": [
        "# Complete the code to pick the 'Label' column as the target variable\n",
        "y_train = X_train[\"Label\"].copy()\n",
        "y_test = X_test[\"Label\"].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imMx6hH0__IB",
      "metadata": {
        "id": "imMx6hH0__IB"
      },
      "outputs": [],
      "source": [
        "print(\"Train data shape\",X_train.shape)\n",
        "print(\"Test data shape \",X_test.shape)\n",
        "\n",
        "print(\"Train label shape\",y_train.shape)\n",
        "print(\"Test label shape \",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0rYgR14ORf7b",
      "metadata": {
        "id": "0rYgR14ORf7b"
      },
      "source": [
        "## **Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4IUBFAOTbjju",
      "metadata": {
        "id": "4IUBFAOTbjju"
      },
      "source": [
        "### **Generating Text Embeddings using Word2Vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Defining the model**"
      ],
      "metadata": {
        "id": "bzwPsqJvVbNC"
      },
      "id": "bzwPsqJvVbNC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZD188ZNsboS4",
      "metadata": {
        "id": "ZD188ZNsboS4"
      },
      "outputs": [],
      "source": [
        "# Creating a list of all words in our data\n",
        "words_list = [item.split(\" \") for item in stock['News'].astype(str).values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eGVgM5iTbwHy",
      "metadata": {
        "id": "eGVgM5iTbwHy"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of Word2Vec\n",
        "vec_size = 300\n",
        "model_W2V = Word2Vec(words_list, vector_size = vec_size, min_count = 1, window=5, workers = 6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the size of the vocabulary\n",
        "print(\"Length of the vocabulary is\", len(list(model_W2V.wv.key_to_index)))"
      ],
      "metadata": {
        "id": "p_Z0J56Z2b70"
      },
      "id": "p_Z0J56Z2b70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Encoding the datasets**"
      ],
      "metadata": {
        "id": "ZYCiT-7GVNaH"
      },
      "id": "ZYCiT-7GVNaH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F_4ldXPzcF7y",
      "metadata": {
        "id": "F_4ldXPzcF7y"
      },
      "outputs": [],
      "source": [
        "# Retrieving the words present in the Word2Vec model's vocabulary\n",
        "words = list(model_W2V.wv.key_to_index.keys())\n",
        "\n",
        "# Retrieving word vectors for all the words present in the model's vocabulary\n",
        "wvs = model_W2V.wv[words].tolist()\n",
        "\n",
        "# Creating a dictionary of words and their corresponding vectors\n",
        "word_vector_dict = dict(zip(words, wvs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Averaging the word vectors to get sentence encodings**"
      ],
      "metadata": {
        "id": "GgismcJz0dZE"
      },
      "id": "GgismcJz0dZE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vsQ0vF42cH_r",
      "metadata": {
        "id": "vsQ0vF42cH_r"
      },
      "outputs": [],
      "source": [
        "def average_vectorizer_Word2Vec(doc):\n",
        "    # Initializing a feature vector for the sentence\n",
        "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
        "\n",
        "    # Creating a list of words in the sentence that are present in the model vocabulary\n",
        "    words_in_vocab = [word for word in doc.split() if word in words]\n",
        "\n",
        "    # adding the vector representations of the words\n",
        "    for word in words_in_vocab:\n",
        "        feature_vector += np.array(word_vector_dict[word])\n",
        "\n",
        "    # Dividing by the number of words to get the average vector\n",
        "    if len(words_in_vocab) != 0:\n",
        "        feature_vector /= len(words_in_vocab)\n",
        "\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jtxc1yVHcJjV",
      "metadata": {
        "id": "Jtxc1yVHcJjV"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe of the vectorized documents\n",
        "start = time.time()\n",
        "\n",
        "X_train_wv = pd.DataFrame(X_train['News'].apply(average_vectorizer_Word2Vec).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "X_test_wv = pd.DataFrame(X_test['News'].apply(average_vectorizer_Word2Vec).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "\n",
        "end = time.time()\n",
        "print('Time taken ', (end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8IrY8tZjA4VZ",
      "metadata": {
        "id": "8IrY8tZjA4VZ"
      },
      "outputs": [],
      "source": [
        "print(X_train_wv.shape, X_test_wv.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3GUvne0hyPx",
      "metadata": {
        "id": "a3GUvne0hyPx"
      },
      "source": [
        "### **Generating Text Embeddings using Sentence Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ITQezWi9VE",
      "metadata": {
        "id": "51ITQezWi9VE"
      },
      "source": [
        "#### **Defining the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3EQ7eQIpYSyz",
      "metadata": {
        "id": "3EQ7eQIpYSyz"
      },
      "outputs": [],
      "source": [
        "#Defining the model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lll4MLfzKfBa",
      "metadata": {
        "id": "Lll4MLfzKfBa"
      },
      "source": [
        "#### **Encoding the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q1BaGKThKcX3",
      "metadata": {
        "id": "q1BaGKThKcX3"
      },
      "outputs": [],
      "source": [
        "# encoding the dataset\n",
        "start = time.time()\n",
        "\n",
        "X_train_st = model.encode(X_train[\"News\"].values, show_progress_bar=True, device=\"cpu\") #Complete the code to apply Sentence Transformer on 'News' column\n",
        "X_test_st = model.encode(X_test[\"News\"].values, show_progress_bar=True, device=\"cpu\") #Complete the code to apply Sentence Transformer on 'News' column\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time taken \",(end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oqi4c8sBC9pH",
      "metadata": {
        "id": "Oqi4c8sBC9pH"
      },
      "outputs": [],
      "source": [
        "print(X_train_st.shape, X_test_st.shape) #Complete the code to print the shapes of the final dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hcGWEfJnBmQ-",
      "metadata": {
        "id": "hcGWEfJnBmQ-"
      },
      "source": [
        "- Each news content has been converted to a 384-dimensional vector."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4lwYN5bYmHp",
      "metadata": {
        "id": "f4lwYN5bYmHp"
      },
      "source": [
        "## **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EKmKPd9LUjnJ",
      "metadata": {
        "id": "EKmKPd9LUjnJ"
      },
      "source": [
        "### **Model Evaluation Criterion**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZkQO_WCCp-pq",
      "metadata": {
        "id": "ZkQO_WCCp-pq"
      },
      "source": [
        "##### **Defining Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TmpiQb6ohfFv",
      "metadata": {
        "id": "TmpiQb6ohfFv"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(actual, predicted):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix to visualize the performance of a classification model.\n",
        "\n",
        "    Parameters:\n",
        "    actual (array-like): The true labels.\n",
        "    predicted (array-like): The predicted labels from the model.\n",
        "\n",
        "    Returns:\n",
        "    None: Displays the confusion matrix plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the confusion matrix.\n",
        "    cm = confusion_matrix(actual, predicted)\n",
        "\n",
        "    # Create a new figure with a specified size\n",
        "    plt.figure(figsize=(5, 4))\n",
        "\n",
        "    # Define the labels for the confusion matrix dynamically from the data\n",
        "    label_list = sorted(list(np.unique(np.concatenate((actual, predicted)))))\n",
        "\n",
        "    # Plot the confusion matrix using a heatmap with annotations\n",
        "    sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues', xticklabels=label_list, yticklabels=label_list)\n",
        "\n",
        "    # Label for the y-axis\n",
        "    plt.ylabel('Actual')\n",
        "\n",
        "    # Label for the x-axis\n",
        "    plt.xlabel('Predicted')\n",
        "\n",
        "    # Title of the plot\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kGJsSn_SKQpP",
      "metadata": {
        "id": "kGJsSn_SKQpP"
      },
      "outputs": [],
      "source": [
        "def model_performance_classification_sklearn(actual, predicted):\n",
        "    \"\"\"\n",
        "    Compute various performance metrics for a classification model using sklearn.\n",
        "\n",
        "    Parameters:\n",
        "    model (sklearn classifier): The classification model to evaluate.\n",
        "    predictors (array-like): The independent variables used for predictions.\n",
        "    target (array-like): The true labels for the dependent variable.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the computed metrics (Accuracy, Recall, Precision, F1-score).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute Accuracy\n",
        "    acc = accuracy_score(actual,predicted)\n",
        "    # Compute Recall\n",
        "    recall = recall_score(actual,predicted,average='weighted')\n",
        "    # Compute Precision\n",
        "    precision = precision_score(actual,predicted,average='weighted')\n",
        "    # Compute F1-score\n",
        "    f1 = f1_score(actual,predicted,average='weighted')\n",
        "\n",
        "    # Create a DataFrame to store the computed metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"Accuracy\": [acc],\n",
        "            \"Recall\": [recall],\n",
        "            \"Precision\": [precision],\n",
        "            \"F1\": [f1],\n",
        "        }\n",
        "    )\n",
        "    # Return the DataFrame with the metrics\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pqg3KJ6TClm1",
      "metadata": {
        "id": "pqg3KJ6TClm1"
      },
      "source": [
        "### **Building Random Forest Models using different text embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Building a Random Forest Model using text embeddings obtained from the Word2Vec**"
      ],
      "metadata": {
        "id": "OyPtD9XriJLx"
      },
      "id": "OyPtD9XriJLx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wKNg-5yJClm-",
      "metadata": {
        "id": "wKNg-5yJClm-"
      },
      "outputs": [],
      "source": [
        "# Build the Random Forest model\n",
        "# Fill in suitable values for:\n",
        "# - n_estimators: the number of trees in the forest (e.g., 100, 200)\n",
        "# - max_depth: maximum depth of each tree (e.g., 10, 20, or None for full depth)\n",
        "rf_word2vec = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf_word2vec.fit(X_train_wv, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking Training and Test Performance**\n"
      ],
      "metadata": {
        "id": "95O3167WbBnd"
      },
      "id": "95O3167WbBnd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on train data\n",
        "y_pred_train = rf_word2vec.predict(X_train_wv)\n",
        "\n",
        "# Predicting on test data\n",
        "y_pred_test = rf_word2vec.predict(X_test_wv)"
      ],
      "metadata": {
        "id": "TtQlY8DlzadF"
      },
      "id": "TtQlY8DlzadF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "ycl7jAX7cZuj"
      },
      "id": "ycl7jAX7cZuj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a_AW25srClm-",
      "metadata": {
        "id": "a_AW25srClm-"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_train,y_pred_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,y_pred_test)"
      ],
      "metadata": {
        "id": "sp4-2sLEDcM3"
      },
      "id": "sp4-2sLEDcM3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Report**"
      ],
      "metadata": {
        "id": "E1jLbrZAidAB"
      },
      "id": "E1jLbrZAidAB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8rV_bYhqClm_",
      "metadata": {
        "id": "8rV_bYhqClm_"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on training data\n",
        "rf_train_wv = model_performance_classification_sklearn(y_train,y_pred_train)\n",
        "print(\"Training performance:\\n\", rf_train_wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_AA2cSvzClm_",
      "metadata": {
        "id": "_AA2cSvzClm_"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on test data\n",
        "rf_test_wv = model_performance_classification_sklearn(y_test, y_pred_test)\n",
        "print(\"Test performance:\\n\",rf_test_wv)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Building a Random Forest Model using text embeddings obtained from the Sentence Transformer**"
      ],
      "metadata": {
        "id": "uijWj2Nl2jyK"
      },
      "id": "uijWj2Nl2jyK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04W4gkoZ2jyK"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "# Fill in suitable values for:\n",
        "# - n_estimators: the number of trees in the forest (e.g., 100, 200)\n",
        "# - max_depth: maximum depth of each tree (e.g., 10, 20, or None for full depth)\n",
        "rf_st = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "\n",
        "\n",
        "# Fitting on train data\n",
        "rf_st.fit(X_train_st, y_train)"
      ],
      "id": "04W4gkoZ2jyK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking Training and Test Performance**"
      ],
      "metadata": {
        "id": "BTWSvJfC2jyL"
      },
      "id": "BTWSvJfC2jyL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on train data\n",
        "y_pred_train = rf_st.predict(X_train_st)\n",
        "\n",
        "# Predicting on test data\n",
        "y_pred_test = rf_st.predict(X_test_st)"
      ],
      "metadata": {
        "id": "QPI_ePlJ2jyL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QPI_ePlJ2jyL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "vskhvTGm2jyL"
      },
      "id": "vskhvTGm2jyL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P_tYSn92jyM"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_train,y_pred_train) #Complete the code to plot the confusion matrix for the train data"
      ],
      "id": "9P_tYSn92jyM"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,y_pred_test) #Complete the code to plot the confusion matrix for the test data"
      ],
      "metadata": {
        "id": "LBzzMFHJDolN"
      },
      "id": "LBzzMFHJDolN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Report**"
      ],
      "metadata": {
        "id": "sSvRSDit2jyM"
      },
      "id": "sSvRSDit2jyM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kEV9XZD2jyM"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on training data\n",
        "rf_train_st = model_performance_classification_sklearn(y_train,y_pred_train)\n",
        "print(\"Training performance:\\n\", rf_train_st)"
      ],
      "id": "_kEV9XZD2jyM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoFxAES32jyM"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on test data\n",
        "rf_test_st = model_performance_classification_sklearn(y_test, y_pred_test)\n",
        "print(\"Test performance:\\n\",rf_test_st)"
      ],
      "id": "QoFxAES32jyM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHgj_cCm2pIn"
      },
      "source": [
        "### **Building Neural Network Models using different text embeddings**"
      ],
      "id": "DHgj_cCm2pIn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Building a Neural Network Model using text embeddings obtained from the Word2Vec**"
      ],
      "metadata": {
        "id": "LpasFYQriueC"
      },
      "id": "LpasFYQriueC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the labels\n",
        "label_mapping = {1: 2, -1: 0, 0: 1}\n",
        "y_train_mapped_wv = [label_mapping[label] for label in y_train]\n",
        "y_test_mapped_wv = [label_mapping[label] for label in y_test]\n",
        "\n",
        "# Convert your features DataFrame to a NumPy array\n",
        "X_train_wv_np = np.array(X_train_wv)\n",
        "X_test_wv_np = np.array(X_test_wv)\n",
        "y_train_mapped_wv = np.array(y_train_mapped_wv)\n",
        "y_test_mapped_wv = np.array(y_test_mapped_wv)"
      ],
      "metadata": {
        "id": "xIeKB-P4nYFi"
      },
      "id": "xIeKB-P4nYFi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Clear any previous TensorFlow/Keras sessions from memory (recommended when re-running cells)\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer:\n",
        "# Choose the number of units (typically in the range of 64 to 512)\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train_wv_np.shape[1],)))  # Input shape = size of Word2Vec embeddings\n",
        "\n",
        "# Dropout layer:\n",
        "# Choose a dropout rate between 0.2 and 0.5 to reduce overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Hidden layer:\n",
        "# Choose number of neurons (again, typically in the range of 32 to 256)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer:\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "# Fill in:\n",
        "# - optimizer: choose from options like 'adam', 'sgd', or 'rmsprop'\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pPoM2BhyXvBv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pPoM2BhyXvBv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "- During training, we use accuracy as a metric to monitor how well the model is learning to distinguish between classes in each batch.\n",
        "- Accuracy is fast and reliable during training and gives us a quick view of model progress.\n",
        "- It reflects how often the model is predicting the correct label out of all predictions made.\n",
        "\n"
      ],
      "metadata": {
        "id": "kIxFfSYLQNlT"
      },
      "id": "kIxFfSYLQNlT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on training data\n",
        "# Fill in appropriate values for:\n",
        "# - epochs: number of times the model will see the entire training data (typically 5 to 50)\n",
        "# - batch_size: number of samples the model will process before updating weights (commonly 16 to 128)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_wv_np, y_train_mapped_wv,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=0 # Suppress output to keep notebook clean\n",
        ")"
      ],
      "metadata": {
        "id": "bgHeOMfpnobV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bgHeOMfpnobV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking Training and Test Performance**"
      ],
      "metadata": {
        "id": "IX11-Hmx8_E1"
      },
      "id": "IX11-Hmx8_E1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities on training data\n",
        "y_train_pred_probs = model.predict(X_train_wv_np, verbose=0)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_train_preds = tf.argmax(y_train_pred_probs, axis=1).numpy()"
      ],
      "metadata": {
        "id": "ZpEpHWni87cO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZpEpHWni87cO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities on test data\n",
        "y_test_pred_probs = model.predict(X_test_wv_np, verbose=0)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_test_preds = tf.argmax(y_test_pred_probs, axis=1).numpy()"
      ],
      "metadata": {
        "id": "hBMMkZBk9Jkz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hBMMkZBk9Jkz"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Convert back to [-1, 0, 1] to match utility function expectations\n",
        "label_mapping = {2: 1, 0: -1, 1: 0}\n",
        "y_train_preds = np.array([label_mapping[index] for index in y_train_preds])\n",
        "y_test_preds = np.array([label_mapping[index] for index in y_test_preds])"
      ],
      "metadata": {
        "id": "wCPqMh0nwryB"
      },
      "id": "wCPqMh0nwryB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "Jbeyf8dzk3MP"
      },
      "id": "Jbeyf8dzk3MP"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_train_mapped_wv, y_train_preds) #Complete the code to plot the confusion matrix for the train data"
      ],
      "metadata": {
        "id": "lIh2fXcwxJ0G"
      },
      "id": "lIh2fXcwxJ0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test_mapped_wv, y_test_preds) #Complete the code to plot the confusion matrix for the test data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "djUVsYwYYBJd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "djUVsYwYYBJd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Report**"
      ],
      "metadata": {
        "id": "1NqSOfNd1UmS"
      },
      "id": "1NqSOfNd1UmS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qzE4NHS1UmS"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on training data\n",
        "NN_train_wv = model_performance_classification_sklearn(y_train,y_train_preds)\n",
        "print(\"Training performance:\\n\", NN_train_wv)"
      ],
      "id": "5qzE4NHS1UmS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nr34HI31UmT"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on test data\n",
        "NN_test_wv = model_performance_classification_sklearn(y_test, y_test_preds)\n",
        "print(\"Test performance:\\n\",NN_test_wv)"
      ],
      "id": "4Nr34HI31UmT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Building a Neural Network Model using text embeddings obtained from the Sentence Transformer**"
      ],
      "metadata": {
        "id": "bcXtMsPu3JfI"
      },
      "id": "bcXtMsPu3JfI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the labels\n",
        "label_mapping = {1: 2, -1: 0, 0: 1}\n",
        "y_train_mapped_st = [label_mapping[label] for label in y_train]\n",
        "y_test_mapped_st = [label_mapping[label] for label in y_test]\n",
        "\n",
        "# Convert your features DataFrame to a NumPy array\n",
        "X_train_st_np = np.array(X_train_st)\n",
        "X_test_st_np = np.array(X_test_st)\n",
        "y_train_mapped_st = np.array(y_train_mapped_st)\n",
        "y_test_mapped_st = np.array(y_test_mapped_st)"
      ],
      "metadata": {
        "id": "FUfjCAua4A2-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FUfjCAua4A2-"
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Clear any previous TensorFlow/Keras sessions from memory (recommended when re-running cells)\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer:\n",
        "# Choose the number of units (typically in the range of 64 to 512)\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train_st.shape[1],)))  # Input shape = size of Word2Vec embeddings\n",
        "\n",
        "# Dropout layer:\n",
        "# Choose a dropout rate between 0.2 and 0.5 to reduce overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Hidden layer:\n",
        "# Choose number of neurons (again, typically in the range of 32 to 256)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer:\n",
        "# 3 output classes → use softmax for multi-class classification\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "# Fill in:\n",
        "# - optimizer: choose from options like 'adam', 'sgd', or 'rmsprop'\n",
        "# - loss: use 'sparse_categorical_crossentropy' for integer-encoded labels\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ziE6DVHA4A2-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ziE6DVHA4A2-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on training data\n",
        "# Fill in appropriate values for:\n",
        "# - epochs: number of times the model will see the entire training data (typically 5 to 50)\n",
        "# - batch_size: number of samples the model will process before updating weights (commonly 16 to 128)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_st_np, y_train_mapped_st,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=0 # Suppress output to keep notebook clean\n",
        ")"
      ],
      "metadata": {
        "id": "8J-JncGj4A2_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8J-JncGj4A2_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking Training and Test Performance**"
      ],
      "metadata": {
        "id": "rbsZ24gM4A2_"
      },
      "id": "rbsZ24gM4A2_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities on training data\n",
        "y_train_pred_probs = model.predict(X_train_st_np, verbose=0)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_train_preds = tf.argmax(y_train_pred_probs, axis=1).numpy()"
      ],
      "metadata": {
        "id": "xaWGws3r4A2_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xaWGws3r4A2_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities on test data\n",
        "y_test_pred_probs = model.predict(X_test_st_np, verbose=0)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_test_preds = tf.argmax(y_test_pred_probs, axis=1).numpy()"
      ],
      "metadata": {
        "id": "P8yF-MWH4A2_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "P8yF-MWH4A2_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert back to [-1, 0, 1] to match utility function expectations\n",
        "label_mapping = {2: 1, 0: -1, 1: 0}\n",
        "y_train_preds = np.array([label_mapping[index] for index in y_train_preds])\n",
        "y_test_preds = np.array([label_mapping[index] for index in y_test_preds])"
      ],
      "metadata": {
        "id": "YbwmP-dE4A3A"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YbwmP-dE4A3A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "YoVyydZW4A3A"
      },
      "id": "YoVyydZW4A3A"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_train_mapped_st,y_train_preds) #Complete the code to plot the confusion matrix for the train data"
      ],
      "metadata": {
        "id": "I2yC2oAB4A3A"
      },
      "execution_count": null,
      "outputs": [],
      "id": "I2yC2oAB4A3A"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test_mapped_st,y_test_preds) #Complete the code to plot the confusion matrix for the test data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mZLu8fRk4A3A"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mZLu8fRk4A3A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Report**"
      ],
      "metadata": {
        "id": "eDqgpX_a4A3B"
      },
      "id": "eDqgpX_a4A3B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq_ES16g4A3B"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on training data\n",
        "NN_train_st = model_performance_classification_sklearn(y_train,y_train_preds)\n",
        "print(\"Training performance:\\n\", NN_train_st)"
      ],
      "id": "Jq_ES16g4A3B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MUEidM44A3B"
      },
      "outputs": [],
      "source": [
        "#Calculating different metrics on test data\n",
        "NN_test_st = model_performance_classification_sklearn(y_test,y_test_preds) #Complete the code to compute the model performance for the test data\n",
        "print(\"Test performance:\\n\",NN_test_st)"
      ],
      "id": "7MUEidM44A3B"
    },
    {
      "cell_type": "markdown",
      "id": "gsmrYpkrFY2A",
      "metadata": {
        "id": "gsmrYpkrFY2A"
      },
      "source": [
        "### **Model Performance Summary and Final Model Selection**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the training performance metrics from different models into a single DataFrame\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_train_wv.T,  # Random Forest using Word2Vec embeddings\n",
        "        NN_train_wv.T,  # Neural Network using Word2Vec embeddings\n",
        "        rf_train_st.T,  # Random Forest using Sentence Transformer embeddings\n",
        "        NN_train_st.T   # Neural Network using Sentence Transformer embeddings\n",
        "    ],\n",
        "    axis=1  # Concatenate along columns (i.e., each model's metrics form one column)\n",
        ")\n",
        "\n",
        "# Assigning meaningful column names for each model for clarity in the output DataFrame\n",
        "models_train_comp_df.columns = [\n",
        "    \"Word2Vec (Random Forest)\",\n",
        "    \"Word2Vec (Neural Network)\",\n",
        "    \"Sentence Transformer (Random Forest)\",\n",
        "    \"Sentence Transformer (Neural Network)\"\n",
        "]\n",
        "\n",
        "# Print the training performance comparison table\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "metadata": {
        "id": "FmgvAlKBWjR-"
      },
      "id": "FmgvAlKBWjR-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the testing performance metrics from different models into a single DataFrame\n",
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_test_wv.T,  # Random Forest using Word2Vec embeddings\n",
        "        NN_test_wv.T,  # Neural Network using Word2Vec embeddings\n",
        "        rf_test_st.T,  # Random Forest using Sentence Transformer embeddings\n",
        "        NN_test_st.T   # Neural Network using Sentence Transformer embeddings\n",
        "    ],\n",
        "    axis=1  # Concatenate along columns so each model's test metrics appear as one column\n",
        ")\n",
        "\n",
        "# Set descriptive column names for clarity in the resulting comparison table\n",
        "models_test_comp_df.columns = [\n",
        "    \"Word2Vec (Random Forest)\",\n",
        "    \"Word2Vec (Neural Network)\",\n",
        "    \"Sentence Transformer (Random Forest)\",\n",
        "    \"Sentence Transformer (Neural Network)\"\n",
        "]\n",
        "\n",
        "# Print the testing performance comparison table\n",
        "print(\"Testing performance comparison:\")\n",
        "models_test_comp_df"
      ],
      "metadata": {
        "id": "APzbgeHrWjOj"
      },
      "id": "APzbgeHrWjOj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model Performance Summary:**"
      ],
      "metadata": {
        "id": "X0yz_T4j6uJc"
      },
      "id": "X0yz_T4j6uJc"
    },
    {
      "cell_type": "markdown",
      "id": "HiOLoD7BO3L-",
      "metadata": {
        "id": "HiOLoD7BO3L-"
      },
      "source": [
        "## **Conclusions and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uAOPv9oQtWcC",
      "metadata": {
        "id": "uAOPv9oQtWcC"
      },
      "source": [
        "<font size=6 color='blue'>Power Ahead</font>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdb68ea3"
      },
      "source": [
        "# Task\n",
        "Prepare data and train the Neural Network model for Sentence Transformer embeddings by executing cells `FUfjCAua4A2-` and `8J-JncGj4A2_`."
      ],
      "id": "bdb68ea3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6134b4aa"
      },
      "source": [
        "## Prepare data for Neural Network (Sentence Transformer)\n",
        "\n",
        "### Subtask:\n",
        "Convert the Sentence Transformer embeddings (`X_train_st`, `X_test_st`) and mapped labels (`y_train`, `y_test`) into NumPy arrays (`X_train_st_np`, `X_test_st_np`, `y_train_mapped_st`, `y_test_mapped_st`).\n"
      ],
      "id": "6134b4aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91f4c052"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to convert the Sentence Transformer embeddings and mapped labels into NumPy arrays. The provided instructions explicitly state to execute the code from cell `FUfjCAua4A2-`.\n",
        "\n"
      ],
      "id": "91f4c052"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39bac572"
      },
      "source": [
        "# Convert the labels\n",
        "label_mapping = {1: 2, -1: 0, 0: 1}\n",
        "y_train_mapped_st = [label_mapping[label] for label in y_train]\n",
        "y_test_mapped_st = [label_mapping[label] for label in y_test]\n",
        "\n",
        "# Convert your features DataFrame to a NumPy array\n",
        "X_train_st_np = np.array(X_train_st)\n",
        "X_test_st_np = np.array(X_test_st)\n",
        "y_train_mapped_st = np.array(y_train_mapped_st)\n",
        "y_test_mapped_st = np.array(y_test_mapped_st)"
      ],
      "id": "39bac572",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3361d5f4"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to convert the Sentence Transformer embeddings and mapped labels into NumPy arrays, as per the instructions to execute cell `FUfjCAua4A2-`.\n",
        "\n"
      ],
      "id": "3361d5f4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eff46f9d"
      },
      "source": [
        "# Convert the labels\n",
        "label_mapping = {1: 2, -1: 0, 0: 1}\n",
        "y_train_mapped_st = [label_mapping[label] for label in y_train]\n",
        "y_test_mapped_st = [label_mapping[label] for label in y_test]\n",
        "\n",
        "# Convert your features DataFrame to a NumPy array\n",
        "X_train_st_np = np.array(X_train_st)\n",
        "X_test_st_np = np.array(X_test_st)\n",
        "y_train_mapped_st = np.array(y_train_mapped_st)\n",
        "y_test_mapped_st = np.array(y_test_mapped_st)"
      ],
      "id": "eff46f9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2132c4a0"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to execute the code from cell `FUfjCAua4A2-` to prepare the data for the Neural Network model using Sentence Transformer embeddings. This involves converting the embeddings and mapped labels into NumPy arrays.\n",
        "\n"
      ],
      "id": "2132c4a0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2044fd4a"
      },
      "source": [
        "# Convert the labels\n",
        "label_mapping = {1: 2, -1: 0, 0: 1}\n",
        "y_train_mapped_st = [label_mapping[label] for label in y_train]\n",
        "y_test_mapped_st = [label_mapping[label] for label in y_test]\n",
        "\n",
        "# Convert your features DataFrame to a NumPy array\n",
        "X_train_st_np = np.array(X_train_st)\n",
        "X_test_st_np = np.array(X_test_st)\n",
        "y_train_mapped_st = np.array(y_train_mapped_st)\n",
        "y_test_mapped_st = np.array(y_test_mapped_st)"
      ],
      "id": "2044fd4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcef863e"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to train the Neural Network model using the prepared Sentence Transformer embeddings. This directly follows the data preparation step, as indicated by the user's overall task and the original notebook structure (cell `8J-JncGj4A2_`).\n",
        "\n"
      ],
      "id": "bcef863e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a7af653"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train_st_np, y_train_mapped_st,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "id": "1a7af653",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "128d14b7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error 'Matrix size-incompatible' indicates that the Neural Network model's input layer was not correctly configured for the Sentence Transformer embeddings. Although there was a cell to define the model for Sentence Transformer embeddings, it appears the model object was not properly updated or rebuilt. To resolve this, the model needs to be re-defined and compiled explicitly with the correct input shape for the Sentence Transformer embeddings (`X_train_st_np.shape[1]`) before fitting.\n",
        "\n"
      ],
      "id": "128d14b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb0419af"
      },
      "source": [
        "import gc\n",
        "\n",
        "# Clear any previous TensorFlow/Keras sessions from memory (recommended when re-running cells)\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer:\n",
        "# Choose the number of units (typically in the range of 64 to 512)\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train_st_np.shape[1],)))  # Corrected input shape to use X_train_st_np.shape[1]\n",
        "\n",
        "# Dropout layer:\n",
        "# Choose a dropout rate between 0.2 and 0.5 to reduce overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Hidden layer:\n",
        "# Choose number of neurons (again, typically in the range of 32 to 256)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer:\n",
        "# 3 output classes → use softmax for multi-class classification\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "# Fill in:\n",
        "# - optimizer: choose from options like 'adam', 'sgd', or 'rmsprop'\n",
        "# - loss: use 'sparse_categorical_crossentropy' for integer-encoded labels\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "id": "eb0419af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51071c67"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Neural Network model has been correctly redefined and compiled with the appropriate input shape for the Sentence Transformer embeddings, the next step is to train this model on the prepared training data. This action directly addresses the `8J-JncGj4A2_` cell in the original notebook.\n",
        "\n"
      ],
      "id": "51071c67"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebf72b14"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train_st_np, y_train_mapped_st,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "id": "ebf72b14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abf668e4"
      },
      "source": [
        "## Evaluate Neural Network Model (Sentence Transformer)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the Neural Network model trained with Sentence Transformer embeddings by generating confusion matrices and classification reports for both training and test datasets.\n"
      ],
      "id": "abf668e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd5ed16"
      },
      "source": [
        "**Reasoning**:\n",
        "To ensure consistency in labels for plotting the confusion matrices, I will first convert the actual labels (`y_train_mapped_st`, `y_test_mapped_st`) from the `[0, 1, 2]` format back to the original `[-1, 0, 1]` format, as the predicted labels (`y_train_preds`, `y_test_preds`) are already in this format. This aligns the labels with how `y_train` and `y_test` are structured for the classification reports.\n",
        "\n"
      ],
      "id": "bcd5ed16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f346ec33"
      },
      "source": [
        "reverse_label_mapping = {2: 1, 0: -1, 1: 0}\n",
        "y_train_actual_original = np.array([reverse_label_mapping[label] for label in y_train_mapped_st])\n",
        "y_test_actual_original = np.array([reverse_label_mapping[label] for label in y_test_mapped_st])\n",
        "\n",
        "# Plot Confusion Matrix for Training Data\n",
        "print(\"Confusion Matrix for Training Data:\")\n",
        "plot_confusion_matrix(y_train_actual_original, y_train_preds)\n",
        "\n",
        "# Plot Confusion Matrix for Test Data\n",
        "print(\"\\nConfusion Matrix for Test Data:\")\n",
        "plot_confusion_matrix(y_test_actual_original, y_test_preds)"
      ],
      "id": "f346ec33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "531d3c58"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the confusion matrices have been plotted, the next step is to calculate and print the classification reports for both training and test data, as per the instructions. This involves using the `model_performance_classification_sklearn` function with the original `y_train` and `y_test` labels and the predicted labels `y_train_preds` and `y_test_preds`.\n",
        "\n"
      ],
      "id": "531d3c58"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0caa91e"
      },
      "source": [
        "#Calculating different metrics on training data\n",
        "NN_train_st = model_performance_classification_sklearn(y_train,y_train_preds)\n",
        "print(\"Training performance:\\n\", NN_train_st)\n",
        "\n",
        "#Calculating different metrics on test data\n",
        "NN_test_st = model_performance_classification_sklearn(y_test,y_test_preds)\n",
        "print(\"Test performance:\\n\",NN_test_st)"
      ],
      "id": "e0caa91e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "664acca8"
      },
      "source": [
        "## Model Performance Summary for Training Data\n",
        "\n",
        "### Subtask:\n",
        "Create a summary table of the performance metrics for all models on the training dataset.\n"
      ],
      "id": "664acca8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eef07185"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create a summary table of training performance metrics. The first instruction is to concatenate the training performance metrics from the 'rf_train_wv', 'NN_train_wv', 'rf_train_st', and 'NN_train_st' DataFrames along the columns.\n",
        "\n"
      ],
      "id": "eef07185"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2913b4fe"
      },
      "source": [
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_train_wv.T,\n",
        "        NN_train_wv.T,\n",
        "        rf_train_st.T,\n",
        "        NN_train_st.T\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "models_train_comp_df.columns = [\n",
        "    \"Word2Vec (Random Forest)\",\n",
        "    \"Word2Vec (Neural Network)\",\n",
        "    \"Sentence Transformer (Random Forest)\",\n",
        "    \"Sentence Transformer (Neural Network)\"\n",
        "]\n",
        "\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "id": "2913b4fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8215cd1f"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'NN_train_wv' is not defined` indicates that the variable `NN_train_wv` was not created. This variable is intended to store the training performance metrics for the Neural Network model using Word2Vec embeddings. The cell responsible for calculating this (`5qzE4NHS1UmS`) was not executed previously. To resolve this, I need to execute that cell to define `NN_train_wv`.\n",
        "\n"
      ],
      "id": "8215cd1f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06269fe5"
      },
      "source": [
        "#Calculating different metrics on training data\n",
        "NN_train_wv = model_performance_classification_sklearn(y_train,y_train_preds)\n",
        "print(\"Training performance:\\n\", NN_train_wv)"
      ],
      "id": "06269fe5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8529111"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `NN_train_wv` variable is defined, I will re-execute the code to concatenate the training performance metrics from all models into a single DataFrame and display it, fulfilling the subtask's instructions.\n",
        "\n"
      ],
      "id": "a8529111"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ba0f61f"
      },
      "source": [
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_train_wv.T,\n",
        "        NN_train_wv.T,\n",
        "        rf_train_st.T,\n",
        "        NN_train_st.T\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "models_train_comp_df.columns = [\n",
        "    \"Word2Vec (Random Forest)\",\n",
        "    \"Word2Vec (Neural Network)\",\n",
        "    \"Sentence Transformer (Random Forest)\",\n",
        "    \"Sentence Transformer (Neural Network)\"\n",
        "]\n",
        "\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "id": "2ba0f61f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4db1ee89"
      },
      "source": [
        "## Model Performance Summary for Test Data\n",
        "\n",
        "### Subtask:\n",
        "Create a summary table of the performance metrics for all models on the test dataset.\n"
      ],
      "id": "4db1ee89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69e5dd9"
      },
      "source": [
        "**Reasoning**:\n",
        "To summarize the test performance of all models, I will concatenate the DataFrames containing the test metrics for each model, assign appropriate column names, print a descriptive header, and then display the resulting summary table.\n",
        "\n"
      ],
      "id": "a69e5dd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "727e66b7"
      },
      "source": [
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_test_wv.T,  # Random Forest using Word2Vec embeddings\n",
        "        NN_test_wv.T,  # Neural Network using Word2Vec embeddings\n",
        "        rf_test_st.T,  # Random Forest using Sentence Transformer embeddings\n",
        "        NN_test_st.T   # Neural Network using Sentence Transformer embeddings\n",
        "    ],\n",
        "    axis=1  # Concatenate along columns so each model's test metrics appear as one column\n",
        ")\n",
        "\n",
        "models_test_comp_df.columns = [\n",
        "    \"Word2Vec (Random Forest)\",\n",
        "    \"Word2Vec (Neural Network)\",\n",
        "    \"Sentence Transformer (Random Forest)\",\n",
        "    \"Sentence Transformer (Neural Network)\"\n",
        "]\n",
        "\n",
        "print(\"Testing performance comparison:\")\n",
        "models_test_comp_df"
      ],
      "id": "727e66b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caab56c2"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'NN_test_wv' is not defined` indicates that the performance metric DataFrame for the Neural Network using Word2Vec embeddings on the test set (`NN_test_wv`) has not been calculated or assigned to a variable. To correctly generate the summary table, I need to ensure that all required performance metrics are computed for their respective models. This involves re-defining and re-training the Neural Network models for both Word2Vec and Sentence Transformer embeddings to get their predictions, then calculating `NN_train_wv`, `NN_test_wv`, `NN_train_st`, and `NN_test_st` using these specific predictions, thereby correctly populating all variables needed for the summary table.\n",
        "\n"
      ],
      "id": "caab56c2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2660bcb"
      },
      "source": [
        "import gc\n",
        "\n",
        "# --- Word2Vec Neural Network ---\n",
        "\n",
        "# Clear any previous TensorFlow/Keras sessions from memory\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "# Define the Word2Vec NN model architecture\n",
        "model_wv_nn = Sequential()\n",
        "model_wv_nn.add(Dense(128, activation='relu', input_shape=(X_train_wv_np.shape[1],)))\n",
        "model_wv_nn.add(Dropout(0.3))\n",
        "model_wv_nn.add(Dense(64, activation='relu'))\n",
        "model_wv_nn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the Word2Vec NN model\n",
        "model_wv_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the Word2Vec NN model on training data\n",
        "model_wv_nn.fit(\n",
        "    X_train_wv_np, y_train_mapped_wv,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=0 # Suppress output to keep notebook clean\n",
        ")\n",
        "\n",
        "# Predict class probabilities on training data for Word2Vec NN\n",
        "y_train_pred_probs_wv = model_wv_nn.predict(X_train_wv_np, verbose=0)\n",
        "y_train_preds_wv = tf.argmax(y_train_pred_probs_wv, axis=1).numpy()\n",
        "\n",
        "# Predict class probabilities on test data for Word2Vec NN\n",
        "y_test_pred_probs_wv = model_wv_nn.predict(X_test_wv_np, verbose=0)\n",
        "y_test_preds_wv = tf.argmax(y_test_pred_probs_wv, axis=1).numpy()\n",
        "\n",
        "# Convert Word2Vec NN predictions back to original labels for metric calculation\n",
        "label_mapping_reverse = {2: 1, 0: -1, 1: 0}\n",
        "y_train_preds_wv_original = np.array([label_mapping_reverse[index] for index in y_train_preds_wv])\n",
        "y_test_preds_wv_original = np.array([label_mapping_reverse[index] for index in y_test_preds_wv])\n",
        "\n",
        "# Calculate metrics for Word2Vec NN\n",
        "NN_train_wv = model_performance_classification_sklearn(y_train, y_train_preds_wv_original)\n",
        "NN_test_wv = model_performance_classification_sklearn(y_test, y_test_preds_wv_original)\n",
        "\n",
        "\n",
        "# --- Sentence Transformer Neural Network ---\n",
        "\n",
        "# Clear any previous TensorFlow/Keras sessions from memory\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "# Define the Sentence Transformer NN model architecture\n",
        "model_st_nn = Sequential()\n",
        "model_st_nn.add(Dense(128, activation='relu', input_shape=(X_train_st_np.shape[1],)))\n",
        "model_st_nn.add(Dropout(0.3))\n",
        "model_st_nn.add(Dense(64, activation='relu'))\n",
        "model_st_nn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the Sentence Transformer NN model\n",
        "model_st_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the Sentence Transformer NN model on training data\n",
        "model_st_nn.fit(\n",
        "    X_train_st_np, y_train_mapped_st,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=0 # Suppress output to keep notebook clean\n",
        ")\n",
        "\n",
        "# Predict class probabilities on training data for Sentence Transformer NN\n",
        "y_train_pred_probs_st = model_st_nn.predict(X_train_st_np, verbose=0)\n",
        "y_train_preds_st = tf.argmax(y_train_pred_probs_st, axis=1).numpy()\n",
        "\n",
        "# Predict class probabilities on test data for Sentence Transformer NN\n",
        "y_test_pred_probs_st = model_st_nn.predict(X_test_st_np, verbose=0)\n",
        "y_test_preds_st = tf.argmax(y_test_pred_probs_st, axis=1).numpy()\n",
        "\n",
        "# Convert Sentence Transformer NN predictions back to original labels for metric calculation\n",
        "y_train_preds_st_original = np.array([label_mapping_reverse[index] for index in y_train_preds_st])\n",
        "y_test_preds_st_original = np.array([label_mapping_reverse[index] for index in y_test_preds_st])\n",
        "\n",
        "# Calculate metrics for Sentence Transformer NN\n",
        "NN_train_st = model_performance_classification_sklearn(y_train, y_train_preds_st_original)\n",
        "NN_test_st = model_performance_classification_sklearn(y_test, y_test_preds_st_original)\n",
        "\n",
        "\n",
        "# --- Generate Summary Table ---\n",
        "\n",
        "# Concatenate the testing performance metrics from different models into a single DataFrame\n",
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_test_wv.T,  # Random Forest using Word2Vec embeddings\n",
        "        NN_test_wv.T,  # Neural Network using Word2Vec embeddings\n",
        "        rf_test_st.T,  # Random Forest using Sentence Transformer embeddings\n",
        "        NN_test_st.T   # Neural Network using Sentence Transformer embeddings\n",
        "    ],\n",
        "    axis=1  # Concatenate along columns so each model's test metrics appear as one column\n",
        ")\n",
        "\n",
        "# Set descriptive column names for clarity in the resulting comparison table\n",
        "models_test_comp_df.columns = [\n",
        "    \"Word2Vec (Random Forest)\",\n",
        "    \"Word2Vec (Neural Network)\",\n",
        "    \"Sentence Transformer (Random Forest)\",\n",
        "    \"Sentence Transformer (Neural Network)\"\n",
        "]\n",
        "\n",
        "# Print the testing performance comparison table\n",
        "print(\"Testing performance comparison:\")\n",
        "models_test_comp_df"
      ],
      "id": "d2660bcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbfb032e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully regenerated all necessary Neural Network models and calculated their performance metrics (`NN_test_wv`, `NN_test_st`). Now that these variables are defined, I can proceed with creating the summary table of test performance metrics for all models as requested by the subtask.\n",
        "\n"
      ],
      "id": "dbfb032e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a08d7d0b"
      },
      "source": [
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_test_wv.T,  # Random Forest using Word2Vec embeddings\n",
        "        NN_test_wv.T,  # Neural Network using Word2Vec embeddings\n",
        "        rf_test_st.T,  # Random Forest using Sentence Transformer embeddings\n",
        "        NN_test_st.T   # Neural Network using Sentence Transformer embeddings\n",
        "    ],\n",
        "    axis=1  # Concatenate along columns so each model's test metrics appear as one column\n",
        ")\n",
        "\n",
        "models_test_comp_df.columns = [\n",
        "    \"Word2Vec (Random Forest)\",\n",
        "    \"Word2Vec (Neural Network)\",\n",
        "    \"Sentence Transformer (Random Forest)\",\n",
        "    \"Sentence Transformer (Neural Network)\"\n",
        "]\n",
        "\n",
        "print(\"Testing performance comparison:\")\n",
        "models_test_comp_df"
      ],
      "id": "a08d7d0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b84208e"
      },
      "source": [
        "## Model Performance Summary and Final Model Selection\n",
        "\n",
        "### Subtask:\n",
        "Analyze the performance summaries for both training and test data across all models to select the best performing model.\n"
      ],
      "id": "4b84208e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "284b3792"
      },
      "source": [
        "**Reasoning**:\n",
        "To analyze the training performance of all models, I will display the `models_train_comp_df` DataFrame, as it contains the summarized performance metrics for each model on the training dataset.\n",
        "\n"
      ],
      "id": "284b3792"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0e259f0"
      },
      "source": [
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "id": "c0e259f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e16a9f62"
      },
      "source": [
        "**Reasoning**:\n",
        "To analyze the test performance of all models, I will display the `models_test_comp_df` DataFrame, as it contains the summarized performance metrics for each model on the test dataset.\n",
        "\n"
      ],
      "id": "e16a9f62"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05dc6a79"
      },
      "source": [
        "print(\"Testing performance comparison:\")\n",
        "models_test_comp_df"
      ],
      "id": "05dc6a79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d3f0a04"
      },
      "source": [
        "### **Model Performance Analysis and Selection**\n",
        "\n",
        "**Training Performance (`models_train_comp_df`):**\n",
        "\n",
        "*   **Random Forest models (Word2Vec and Sentence Transformer)** show very high accuracy, recall, precision, and F1-scores (all around 0.87-0.89). This indicates that they are performing well on the training data.\n",
        "*   **Neural Network models (Word2Vec and Sentence Transformer)** show significantly lower training performance, with F1-scores around 0.48-0.79. The Word2Vec NN performs particularly poorly on training data.\n",
        "\n",
        "**Test Performance (`models_test_comp_df`):**\n",
        "\n",
        "*   **Random Forest models (Word2Vec and Sentence Transformer)** maintain similar F1-scores on the test set (around 0.65) compared to each other, but there's a notable drop from their training performance, suggesting some overfitting.\n",
        "*   **Neural Network model with Word2Vec embeddings** has an F1-score of around 0.58 on the test set, which is an improvement over its training performance but still relatively low.\n",
        "*   **Neural Network model with Sentence Transformer embeddings** shows the best performance on the test set among all models, with an F1-score of 0.709. Its test performance (Accuracy: 0.747, Recall: 0.747, Precision: 0.725) is higher than its training performance, which might indicate that the model is generalizing well, or that the training metrics earlier were not fully representative due to the limited number of epochs.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Based on the test set performance, the **Neural Network model using Sentence Transformer embeddings** (`NN_test_st`) appears to be the most suitable model for this sentiment analysis task. It achieves the highest F1-score (0.709) and accuracy (0.747) on the unseen test data. While the Random Forest models showed excellent training performance, their performance significantly dropped on the test set, indicating potential overfitting or less robust generalization compared to the Neural Network with Sentence Transformers. The Neural Network with Word2Vec embeddings did not perform as well on either training or test sets."
      ],
      "id": "7d3f0a04"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}